{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saving and Loading model.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "copyright",
        "SfNRd6-xwNqb",
        "0tbZzEGMxJkN"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rizkimul/Aritmatika/blob/master/Saving_and_Loading_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlO4Bu21oZ4"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlzIlBsScJJ_"
      },
      "source": [
        "# Saving and Loading Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTirVS4FWaPx"
      },
      "source": [
        "So far we have created models and immediately used them for prediction. This is useful in a classroom setting, but most of the time you will need to deploy your model to some production system. You might also want to use one of the many pre-trained models available online in your own project.\n",
        " \n",
        "In this Colab we will save and load models in both scikit-learn and TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1c_USh1M6QY"
      },
      "source": [
        "## scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riNd35zrNOSG"
      },
      "source": [
        "### Build a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdxZJqtqNN8x"
      },
      "source": [
        "To begin, we will build a simple linear regression model using scikit-learn.\n",
        " \n",
        "The dataset we are using is one of the datasets that comes packaged with scikit-learn, called [linnerud](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_linnerud.html#sklearn.datasets.load_linnerud). The dataset contains 20 observations of different people's ability to perform chin-ups, situps, and jumping jacks, along with measurements of their weight, waist, and resting pulse.\n",
        " \n",
        "We'll create a [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model that predicts resting pulse from the other variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVnvw98_OgrM"
      },
      "source": [
        "from sklearn.datasets import load_linnerud\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "bunch = load_linnerud()\n",
        "df = pd.DataFrame(\n",
        "  np.c_[(bunch.data, bunch.target)],\n",
        "  columns=np.concatenate((bunch.feature_names, bunch.target_names))\n",
        ")\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(df[[\"Chins\", \"Situps\", \"Jumps\"]], df[\"Pulse\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siGXLJ_4RDmy"
      },
      "source": [
        "### Save the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQitCr63RFkf"
      },
      "source": [
        "Now that we have built a model, let's save it. Saving a model is also known as \"persisting\" a model. You can learn more about persisting models [on scikit-learn's documentation](https://scikit-learn.org/stable/modules/model_persistence.html).\n",
        "\n",
        "The scikit-learn library doesn't have any built-in ability to save models, so it delegates that work to other tools, such as Python's core [pickle](https://docs.python.org/3/library/pickle.html) module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChDv4ywhM6EM"
      },
      "source": [
        "import pickle\n",
        "\n",
        "model_file = 'my_regression.pkl'\n",
        "\n",
        "with open(model_file, 'wb') as output:\n",
        "  pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZQIyHZISZjf"
      },
      "source": [
        "After running the code block above, you should be able to click on the file browsing tab in Colab and see a new file called `my_regression.pkl`. This file contains your saved model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBXgLAwHSlyN"
      },
      "source": [
        "### Load the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eajAO1k5SpKw"
      },
      "source": [
        "We will now load our saved model, which we can then use to predict on new data (as seen in the cell below) or train it with additional data. To do this we rely on the pickle module again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNAWjcGzS3QP"
      },
      "source": [
        "with open(model_file, 'rb') as input:\n",
        "  model_restored = pickle.load(input)\n",
        "\n",
        "model_restored.predict([[45, 34, 2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVjj2Ez7Tj4K"
      },
      "source": [
        "## TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7RigNRIBn_r"
      },
      "source": [
        "### Building and Saving Your Own Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MvjU7KsBqdb"
      },
      "source": [
        "It is possible to build and save your own model using TensorFlow 2. To demonstrate, let's create a simple model using the [linnerud](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_linnerud.html#sklearn.datasets.load_linnerud) that we used in our earlier example.\n",
        "\n",
        "Note that we aren't following best practices for model building here. We don't split the data, we completely skip EDA, etc. This model is just being built so we can save and reload it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIrgKRPVBxms"
      },
      "source": [
        "from sklearn.datasets import load_linnerud\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "bunch = load_linnerud()\n",
        "\n",
        "df = pd.DataFrame(\n",
        "  np.c_[(bunch.data, bunch.target)],\n",
        "  columns=np.concatenate((bunch.feature_names, bunch.target_names))\n",
        ")\n",
        "\n",
        "features = df.columns.values[:-1]\n",
        "target = df.columns.values[-1]\n",
        "\n",
        "df[features] = ((df[features] - df[features].min()) / \n",
        "                (df[features].max() - df[features].min()))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(16, input_shape=[len(features)], \n",
        "                        activation=tf.nn.leaky_relu),\n",
        "  tf.keras.layers.Dense(8, activation=tf.nn.leaky_relu),\n",
        "  tf.keras.layers.Dense(1, activation=tf.nn.leaky_relu),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  optimizer='Adam',\n",
        "  loss='mse',\n",
        "  metrics=['mse'],\n",
        ")\n",
        "\n",
        "model.fit(df[features], df[target], epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCLw3zc9K_SC"
      },
      "source": [
        "Saving the model is as easy as calling [`save_model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZX08dkVGhIx"
      },
      "source": [
        "tf.keras.models.save_model(\n",
        "  model, 'linnerud'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NRB2js4LML1"
      },
      "source": [
        "We can then view the model file. The file is in TensorFlow format, but it can be saved as H5, which is another popular model storage format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2LVIK3yGhGP"
      },
      "source": [
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Em2uW7-LRbd"
      },
      "source": [
        "Now we can reload the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt7zYl9sKnxA"
      },
      "source": [
        "loaded_model = tf.keras.models.load_model(\n",
        "  'linnerud'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-rZtfQuLhZ5"
      },
      "source": [
        "And then use it to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mahh1CjaKvxt"
      },
      "source": [
        "loaded_model.predict(df.loc[0:2, features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVxNXz0cLrla"
      },
      "source": [
        "Or even train some more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtxMoA_2LnDY"
      },
      "source": [
        "loaded_model.fit(df[features], df[target], epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9aMFHaC_x-v"
      },
      "source": [
        "### Using a Pre-Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD9qiB3C_0jZ"
      },
      "source": [
        "Building and training a model from scratch can be a long and painful process. Luckily there are many publicly available models that are pre-trained and hosted online for you to freely use.\n",
        " \n",
        "But why would you want to use a pre-trained model?\n",
        "\n",
        "There may already be a model that performs some task you need to perform, such as image recognition. Or you may want to build on an existing model, utilizing [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) to adapt the model to your problem.\n",
        " \n",
        "Where can you find these models?\n",
        "\n",
        "One of the most common places is the [official list](https://github.com/tensorflow/models) of pre-trained models curated by TensorFlow developers. This is often referred to as the *TensorFlow Model Garden*.\n",
        " \n",
        "In this course we will be utilizing models stored in the TensorFlow detection model zoo. The zoo has models built with [TensorFlow 1](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) and [TensorFlow 2](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). We'll be focusing on the [Common Objects in Context (COCO)](http://cocodataset.org/) dataset. This dataset contains over 270,000 labeled images in 91 categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUHykGkHCZpO"
      },
      "source": [
        "#### Obtain the Model File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRW_C4qMCc8z"
      },
      "source": [
        "In order to use a pre-trained model, we first need to obtain the model file. For this Colab we'll visit the [TensorFlow detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) and download the `ssd_mobilenet_v1_coco` model.\n",
        "\n",
        "The direct link to the model changes as the model is updated, so you'll need to browse the links in the zoo and find the model. Once you click the download link, you'll have a file on your system named similarly to (but not necessarily exactly the same as):\n",
        "\n",
        " > `ssd_mobilenet_v1_coco_2018_01_28.tar.gz`\n",
        "\n",
        "This is a compressed version of the model file. It is a gzipped (`.gz`) tape archive (`.tar`) file. If you want to explore the file on your local system, you might need to install a program such as [7-zip](https://www.7-zip.org/). On Mac and Linux systems you should be able to right-click on the file and extract the contents without any extra software. If you are comfortable with the command line, you can use the following command to extract the file contents.\n",
        "\n",
        "  > `tar -xzvf ssd_mobilenet_v1_coco_2018_01_28.tar.gz`\n",
        "\n",
        "And finally, if you just want to directly load the file to this Colab, update the file name in the code snippet below and run the code.\n",
        "\n",
        "Also notice that the documentation for the model we are downloading says the model was built using TensorFlow version 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqpzWYtcFTmk"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
        "file_name = 'ssd_mobilenet_v1_coco_2018_01_28.tar.gz'\n",
        "\n",
        "url = base_url + file_name\n",
        "\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuxLOHIJjBEU"
      },
      "source": [
        "#### Extract the Model Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsfJLTVxSq7X"
      },
      "source": [
        "In order to load the model, it must be extracted from the compressed archive file (also called a \"tarball\" in this case). We will use Python's `tarfile` module to extract the contents of the file. The contents of the file will be saved in a directory named after the file. For example, the contents of `ssd_mobilenet_v1_coco_2018_01_28.tar.gz` will be saved in the `ssd_mobilenet_v1_coco_2018_01_28` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WGXDh3N8Nsr"
      },
      "source": [
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "dir_name = file_name[0:-len('.tar.gz')]\n",
        "\n",
        "if os.path.exists(dir_name):\n",
        "  shutil.rmtree(dir_name) \n",
        "\n",
        "tarfile.open(file_name, 'r:gz').extractall('./')\n",
        "\n",
        "os.listdir(dir_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1xn4IomjGf2"
      },
      "source": [
        "#### Load the Frozen Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxOEoVzQUIIX"
      },
      "source": [
        "There are some interesting files contained in the archive, including checkpoints that can be used for resuming model training from a specific point. We care mostly about the `frozen_inference_graph.pb` file; this file contains a trained TensorFlow graph we can use for classification.\n",
        "\n",
        "We can load the frozen graph using TensorFlow's `GFile` method to open the file, then call `GraphDef.ParseFromString` to load the graph into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-g_Wmbs-FKB"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "frozen_graph = os.path.join(dir_name, 'frozen_inference_graph.pb')\n",
        "\n",
        "with tf.io.gfile.GFile(frozen_graph, \"rb\") as f:\n",
        "  graph_def = tf.compat.v1.GraphDef()\n",
        "  loaded = graph_def.ParseFromString(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqS97qcwjNXd"
      },
      "source": [
        "#### Explore the Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RaKacjGbc8T"
      },
      "source": [
        "Now we have a graph loaded! But how do we use it? What are the inputs and outputs?\n",
        " \n",
        "In a perfect world, model builders would fully document the inputs and outputs of their models, and we would know exactly how to interact with them. Unfortunately, this is not a perfect world.\n",
        " \n",
        "The first step is just to determine the number of nodes in the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjl6bZBieAPH"
      },
      "source": [
        "len(graph_def.node)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HiLpHl7eIWq"
      },
      "source": [
        "We have about 5,960 nodes. This is probably more than you would want to sort through manually.\n",
        " \n",
        "TensorFlow automatically names nodes if you don't name them yourself. Nested nodes typically have a slash, '/', in the name.\n",
        " \n",
        "How many nodes do we have without slashes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewa4ZkGHeoEn"
      },
      "source": [
        "len([n for n in graph_def.node if '/' not in n.name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgnCFzsker7L"
      },
      "source": [
        "That is a much more approachable number. Let's print out these \"edge\" nodes and see if we can see any obvious inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC-uML7xe1DF"
      },
      "source": [
        "for n in graph_def.node:\n",
        "  if '/' not in n.name:\n",
        "    print(n.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEGqECjQfDoz"
      },
      "source": [
        "At the time of creating this Colab, the edge node list was:\n",
        " \n",
        " * image_tensor\n",
        " * ToFloat\n",
        " * Shape_6\n",
        " * strided_slice_6\n",
        " * strided_slice_7\n",
        " * concat\n",
        " * Squeeze\n",
        " * concat_1\n",
        " * detection_boxes\n",
        " * detection_scores\n",
        " * num_detections\n",
        " * add\n",
        " * detection_classes\n",
        " \n",
        "Many of these look computer-generated, or like it wouldn't really matter if you knew their value or not. The notable nodes seem to be:\n",
        " \n",
        " * image_tensor\n",
        " * detection_boxes\n",
        " * detection_scores\n",
        " * num_detections\n",
        " * detection_classes\n",
        " \n",
        "*image_tensor* looks a lot like an input, and the *detection* classes look like output from a classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kI2weUhglGv"
      },
      "source": [
        "Let's take a look at the details of the *image_tensor*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVjR_DTDgQhT"
      },
      "source": [
        "for n in graph_def.node:\n",
        "  if n.name == 'image_tensor':\n",
        "    print(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbRDU4dQgyDK"
      },
      "source": [
        "*Placeholder* seems significant; thinking back to our \"Introduction to TensorFlow\" lesson, we remember that placeholders are the entrypoint for input data into the graph.\n",
        " \n",
        "Let's see how many placeholders we have in this graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6kuZR14gpw1"
      },
      "source": [
        "for n in graph_def.node:\n",
        "  if n.op == 'Placeholder':\n",
        "    print(n.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2nk2ZCmhWcz"
      },
      "source": [
        "We have only one: *image_tensor*.\n",
        "\n",
        "What size input does the input tensor take?\n",
        "\n",
        "Look back a few cells, and you'll see that it expects input of shape (-1, -1, -1, 3). The -1 values just indicate that the tensor accepts variable sized input, but the 3 is telling. RGB values are three values containing colors of an image. This tensor likely wants a variable number of images -- with a variable width and height -- but with three color values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDQvEpwgigTP"
      },
      "source": [
        "Now let's look at our *detection* nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4td5SfFmiBRD"
      },
      "source": [
        "for n in graph_def.node:\n",
        "  if 'detection' in n.name and '/' not in n.name:\n",
        "    print(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-5b5cxvik2X"
      },
      "source": [
        "We can see that each node receives a float value, but we're not sure what shape the data will have.\n",
        " \n",
        "However, we know the data that we need to input into the graph and the data that we can extract from the graph. We can run the graph and manually explore the outputs to determine what data and shape they conform to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptRrA7OGjzQD"
      },
      "source": [
        "#### Run the Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0U0a6xmV3jw"
      },
      "source": [
        "Now that we have a graph loaded, we need to test it out. Let's download an [image of a car](https://pixabay.com/illustrations/car-sports-car-racing-car-speed-49278/) and upload that image to Colab. Rename the file `car.jpg` or change the name of the `image_filename` variable below to match the name of the file you uploaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wb53z9mWOWz"
      },
      "source": [
        "image_filename = 'car.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRs6HG33WWz3"
      },
      "source": [
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image = cv.imread('car.jpg')\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujy6B5mtkxjv"
      },
      "source": [
        "Remember that TensorFlow allows you to request execution to any node in the graph. We want to know all of the *detection* outputs that we discovered in the graph. These were:\n",
        "\n",
        "  * num_detections\n",
        "  * detection_scores\n",
        "  * detection_boxes\n",
        "  * detection_classes\n",
        "\n",
        "We will build a list of *outputs* that we want TensorFlow to generate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzQQ1HLplnqJ"
      },
      "source": [
        "outputs = (\n",
        "  'num_detections:0',\n",
        "  'detection_classes:0',\n",
        "  'detection_scores:0',\n",
        "  'detection_boxes:0',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYtBjWbomK8f"
      },
      "source": [
        "We also need to input our data into the graph. Our expected input shape is (-1, -1, -1, 3). Let's see the shape of our input image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Cp8bA9mWg8"
      },
      "source": [
        "image.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJb1rURLmZE6"
      },
      "source": [
        "(360, 640, 3) is a 3-dimensional shape, while our model is expecting a 4-dimensional input. This is because the model can accept multiple images to train on, and we have only one image. All we need to do is wrap the image in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl3Sje_QmrO0"
      },
      "source": [
        "input_images = [image]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ebFK1dUmxR7"
      },
      "source": [
        "We can now execute the graph requesting our outputs and providing inputs.\n",
        "\n",
        "In order to do this, we must first wrap the graph. This is necessary due to compatibility issues between TensorFlow version 1 and 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D-ZWbrbxM54"
      },
      "source": [
        "def wrap_graph(graph_def, inputs, outputs, print_graph=False):\n",
        "  wrapped = tf.compat.v1.wrap_function(\n",
        "    lambda: tf.compat.v1.import_graph_def(graph_def, name=\"\"), [])\n",
        "\n",
        "  return wrapped.prune(\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, inputs),\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, outputs))\n",
        "    \n",
        "model = wrap_graph(graph_def=graph_def,\n",
        "                   inputs=[\"image_tensor:0\"],\n",
        "                   outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01N8Tgky1qAD"
      },
      "source": [
        "We can now look at the input placeholder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEhNu1wpyx1Y"
      },
      "source": [
        "model.inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0gLTD3Q1t4h"
      },
      "source": [
        "And the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sJHMff9yz0f"
      },
      "source": [
        "model.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIrNemK610Gp"
      },
      "source": [
        "And then to make predictions, we convert our image into a tensor and pass it to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdB5SPDPzBTh"
      },
      "source": [
        "tensor = tf.convert_to_tensor(input_images, dtype=tf.uint8)\n",
        "\n",
        "detections = model(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPBzdQnTnri7"
      },
      "source": [
        "#### Investigate Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v83xexNPnid3"
      },
      "source": [
        "Now we can explore the outputs of that graph.\n",
        "\n",
        "When the graph executed, it stored all outputs in a variable called `detections`. Let's first look at that variable and see what type it is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzIqOHjhndlz"
      },
      "source": [
        "type(detections)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl2ps62roC58"
      },
      "source": [
        "A tuple. How many elements?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUjbaxjioKdA"
      },
      "source": [
        "len(detections)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H81aP3yPoQCy"
      },
      "source": [
        "Four. That is suspiciously similar to the number of outputs we asked for. Remember that we requested:\n",
        "\n",
        "  1. num_detections\n",
        "  1. detection_classes\n",
        "  1. detection_scores\n",
        "  1. detection_boxes\n",
        "\n",
        "Let's see if our first row looks like the number of detections that were found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPaiQgGtoqdK"
      },
      "source": [
        "detections[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB9eyZRso0Km"
      },
      "source": [
        "This makes sense because we sent the classifier only one image, and it was of a car.\n",
        " \n",
        "Let's move to the next list item."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqd_vxQco-4U"
      },
      "source": [
        "detections[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D5hCCHepDPz"
      },
      "source": [
        "We get a list of a list where all of the values of the sub-list are 1 except for the first value.\n",
        "\n",
        "The second output we requested was 'classes'. This is likely the index of the class of the detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SebqXuuqrR8V"
      },
      "source": [
        "And the next element?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdI5fbNTqF6a"
      },
      "source": [
        "detections[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik2OHeJPralA"
      },
      "source": [
        "This element is the scores we expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_4jP_l8rglb"
      },
      "source": [
        "Now for the final element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdKVmVWSriKM"
      },
      "source": [
        "detections[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu1qZpucrn5L"
      },
      "source": [
        "Four-element lists. These look a lot like bounding boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvlTLftsrrhJ"
      },
      "source": [
        "We inspected our model and found that it accepts a list of variable-sized images and that it returns:\n",
        "- the number of matches\n",
        "- the class\n",
        "- the confidence\n",
        "- the bounding boxes for each object found in an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ar7tuYPsEqP"
      },
      "source": [
        "#### Mapping Numbers to Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjvqT77dsILb"
      },
      "source": [
        "We now have enough information to know how to find, load, and use a pre-trained classification model. But how do we know what the discovered classes actually mean?\n",
        " \n",
        "Typically in machine learning models, a class is represented as a number. We have to work outside of the model to turn that number back into a label that makes sense to a human.\n",
        " \n",
        "Typically this takes a bit of searching. In the case of the model we just used, you can find the label-to-number mapping in a [textual protocol buffer file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) on GitHub.\n",
        " \n",
        "It is the responsibility of the model user to find the number-to-text mappings and apply those to the model output. In this case we can see that classification 3 is a car."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVSiU72yz9SD"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIOgOHP1ces"
      },
      "source": [
        "## Exercise 1: scikit-learn Pickling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTEOK1ZmqN8"
      },
      "source": [
        "In this exercise you will build an [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) using the [linnerud](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_linnerud.html#sklearn.datasets.load_linnerud) dataset and train the model to predict `Pulse`.\n",
        "\n",
        "Create the regression model, and then pickle it into a file called `sgd_reg_linnerud.pkl`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XM35vYWSbim"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivTzfzQN5jDk"
      },
      "source": [
        "import pickle\n",
        "\n",
        "model_file = 'sgd_reg_linnerud.pkl'\n",
        "\n",
        "with open(model_file, 'wb') as output:\n",
        "  pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "bunch = load_linnerud()\n",
        "df = pd.DataFrame(\n",
        "  np.c_[(bunch.data, bunch.target)],\n",
        "  columns=np.concatenate((bunch.feature_names, bunch.target_names))\n",
        ")\n",
        "x_col = [i for i in df.columns if i != 'Pulse']\n",
        "x = df[x_col]\n",
        "y = df['Pulse']\n",
        "\n",
        "model = SGDRegressor()\n",
        "model.fit(x,y)\n",
        "\n",
        "import pickle\n",
        "\n",
        "model_file = 'sgd_reg_linnerud.pkl'\n",
        "\n",
        "with open(model_file, 'wb') as output:\n",
        "  pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yoq9f2iReDS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t_yR_4Ywhw9"
      },
      "source": [
        "## Exercise 2: Using a Pre-Trained TensorFlow Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiHc5ZAIwpG3"
      },
      "source": [
        "In this exercise you'll load the COCO dataset used above into a graph. Then using [this photo from Pixabay](https://pixabay.com/photos/pets-cute-cat-dog-cute-wallpaper-3715733/), download the photo and draw a green box around every dog and a blue box around every cat.\n",
        "\n",
        "Save the modified image as 'pets.jpg'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADl6ZjZ-xGgP"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPLKb2bBxH8L"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RzKmFKJR1cg"
      },
      "source": [
        "---"
      ]
    }
  ]
}